{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2272,
     "status": "ok",
     "timestamp": 1616449682635,
     "user": {
      "displayName": "Jose Maria Ramos Ugalde",
      "photoUrl": "",
      "userId": "00146683511351227880"
     },
     "user_tz": 360
    },
    "id": "iCIxiVH5Rycs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 100364,
     "status": "ok",
     "timestamp": 1616449780742,
     "user": {
      "displayName": "Jose Maria Ramos Ugalde",
      "photoUrl": "",
      "userId": "00146683511351227880"
     },
     "user_tz": 360
    },
    "id": "A0Ov27pKS3Rt"
   },
   "outputs": [],
   "source": [
    "# We import a trained model\n",
    "loaded_model = load_model('../Saved_models/MIXED.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100359,
     "status": "ok",
     "timestamp": 1616449780743,
     "user": {
      "displayName": "Jose Maria Ramos Ugalde",
      "photoUrl": "",
      "userId": "00146683511351227880"
     },
     "user_tz": 360
    },
    "id": "0Xk8de_iS3Xn",
    "outputId": "0db11fe1-ccb2-462d-ae2d-8efa025e6c7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 100)         1861100   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 256)         365568    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 256)         525312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 256)         525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 18611)             4783027   \n",
      "=================================================================\n",
      "Total params: 8,585,631\n",
      "Trainable params: 8,585,631\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Check the model structure\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We  need to import the text which trained our model in order to create the same dictionary when generating the model\n",
    "with open ('../../Data/Data_clean_txt/mixed_text.txt', encoding='utf-8-sig') as f:\n",
    "    text = f.read()\n",
    "seq_length = 20\n",
    "# check the indexing used for each model\n",
    "text = text[:int(len(text)/2.75)]\n",
    "start_story = '| ' * seq_length\n",
    "text = start_story + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"This function will clean a text in order to make it suitable for the LSTM model\"\"\"\n",
    "    #text = text.lower()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub('   +', '. ', text).strip()\n",
    "    text = text.replace('..', '.')\n",
    "    #text = text.replace('| ', '')\n",
    "    # This pattern will insert a space before any punctuation sign in order to also tokenize them\n",
    "    text = re.sub('([!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~])', r' \\1 ', text)\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 101777,
     "status": "ok",
     "timestamp": 1616449782173,
     "user": {
      "displayName": "Jose Maria Ramos Ugalde",
      "photoUrl": "",
      "userId": "00146683511351227880"
     },
     "user_tz": 360
    },
    "id": "KRU4IVQUS3xN"
   },
   "outputs": [],
   "source": [
    "#Â We create a Tokenizer object\n",
    "# this object must have lowercase true or false depending on how the model was trained\n",
    "tokenizer = Tokenizer(char_level = False, filters = '', lower = False)\n",
    "# We fit our object with our text\n",
    "tokenizer.fit_on_texts([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 114671,
     "status": "ok",
     "timestamp": 1616449795082,
     "user": {
      "displayName": "Jose Maria Ramos Ugalde",
      "photoUrl": "",
      "userId": "00146683511351227880"
     },
     "user_tz": 360
    },
    "id": "FAY1UumCzAMl"
   },
   "outputs": [],
   "source": [
    "def sample_with_temp(preds, temperature=1.0):\n",
    "    \"\"\"This function will generate predictions using our generativemodel given a specific temperature\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    # creates a random experiment given the probabilities for ou next word\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    # return the word with more probability in our experiment \n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len, temp):\n",
    "    \"\"\"This function will generate a text of a given size using the predictions created by our model\"\"\"\n",
    "    output_text = seed_text\n",
    "    \n",
    "    seed_text = start_story + seed_text\n",
    "    \n",
    "    for _ in range(next_words):\n",
    "      # Tokenize the seed text\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        # We trim the seed text and the start text in order to be of length 20 words\n",
    "        token_list = token_list[-max_sequence_len:]\n",
    "        # we turn the tokenized words list into a np array\n",
    "        token_list = np.reshape(token_list, (1, max_sequence_len))\n",
    "        \n",
    "        # get the predictions\n",
    "        probs = model.predict(token_list, verbose=0)[0]\n",
    "        # get the most probable next word \n",
    "        y_class = sample_with_temp(probs, temperature = temp)\n",
    "        \n",
    "        # if probability = 0 returns no word\n",
    "        if y_class == 0:\n",
    "            output_word = ''\n",
    "        # if the probability is not 0     \n",
    "        else:\n",
    "            output_word = tokenizer.index_word[y_class]\n",
    "            \n",
    "        output_text += output_word + ' '\n",
    "        seed_text += output_word + ' '\n",
    "\n",
    "    return output_text\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP7NH+n2cZsjsRHNH+UFEyn",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "3lstm_ModelLoad_adam_lowercase_Mixed",
   "provenance": [
    {
     "file_id": "1uPbeFUpgaE6tJUJmA_iQVrWBFzzWgJsn",
     "timestamp": 1616378852538
    },
    {
     "file_id": "1o348ZxCuP_HSA0rkeciLMFySB-7TgIzS",
     "timestamp": 1616347940172
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
